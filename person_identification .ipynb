{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "person_identification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deepeshgrover/Third_Eye/blob/main/person_identification%20.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRTa3Ee15WsJ"
      },
      "source": [
        "# Person identification using Transfer Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBMcobPHdD8O"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOG3l_MsBO1A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "48b3425e-0345-4e07-8bee-b927f9004b67"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.4.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4QOy2uA3P_p"
      },
      "source": [
        "Mount the persons dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4gTv7ig2vMh"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0C5cjembp9L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9f8e0b7-829c-4e59-887d-8ded8ec4f9bf"
      },
      "source": [
        "\n",
        "!git clone https://github.com/deepeshgrover/ThirdEye.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ThirdEye'...\n",
            "remote: Enumerating objects: 246, done.\u001b[K\n",
            "remote: Total 246 (delta 0), reused 0 (delta 0), pack-reused 246\u001b[K\n",
            "Receiving objects: 100% (246/246), 163.62 MiB | 46.70 MiB/s, done.\n",
            "Resolving deltas: 100% (27/27), done.\n",
            "Checking out files: 100% (254/254), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vwWzSmMj4-ZM"
      },
      "source": [
        "Use `ImageDataGenerator` to rescale the images.\n",
        "\n",
        "Create the train generator and specify where the train dataset directory, image size, batch size.\n",
        "\n",
        "Create the validation generator with similar approach as the train generator with the flow_from_directory() method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxL2mjVVGIrV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "409faf21-9358-4be2-afd8-6121fd154696"
      },
      "source": [
        "DataDir = '/content/Third_Eye/'\n",
        "\n",
        "\n",
        "\n",
        "base_dir = os.path.join(os.path.dirname(DataDir), 'person_identification')\n",
        "\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "\n",
        "# Directory with our training akhil pictures\n",
        "train_akhil_dir = os.path.join(train_dir, 'Akhil')\n",
        "\n",
        "# Directory with our training deepesh pictures\n",
        "train_deepesh_dir = os.path.join(train_dir, 'Deepesh')\n",
        "\n",
        "# Directory with our training Harsh pictures\n",
        "train_harsh_dir = os.path.join(train_dir, 'Harsh')\n",
        "\n",
        "# Directory with our training Mohit pictures\n",
        "train_mohit_dir = os.path.join(train_dir, 'Mohit')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Directory with our validation akhil pictures\n",
        "validation_akhil_dir = os.path.join(validation_dir, 'Akhil')\n",
        "\n",
        "# Directory with our validation deepesh pictures\n",
        "validation_deepesh_dir = os.path.join(validation_dir, 'Deepesh')\n",
        "\n",
        "# Directory with our validation harsh pictures\n",
        "validation_harsh_dir = os.path.join(validation_dir, 'Harsh')\n",
        "\n",
        "# Directory with our validation mohit pictures\n",
        "validation_mohit_dir = os.path.join(validation_dir, 'Mohit')\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "IMAGE_SIZE = 224\n",
        "IMG_SHAPE = (IMAGE_SIZE, IMAGE_SIZE, 3)\n",
        "\n",
        "# Create the base model from the pre-trained model MobileNet V2\n",
        "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
        "                                              include_top=False, \n",
        "                                              weights='imagenet')\n",
        "### Feature extraction\n",
        "#You will freeze the convolutional base created from the previous step and use that as a feature extractor, \n",
        "#add a classifier on top of it and train the top-level classifier.\n",
        "base_model.trainable = False\n",
        "model = tf.keras.Sequential([\n",
        "  base_model,\n",
        "  tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.GlobalAveragePooling2D(),\n",
        "  tf.keras.layers.Dense(4, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(), \n",
        "              loss='categorical_crossentropy', \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# All images will be rescaled by 1./255\n",
        "train_datagen = ImageDataGenerator(rescale=1./255,\n",
        "      rotation_range=40,\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      shear_range=0.2,\n",
        "      zoom_range=0.2,\n",
        "      horizontal_flip=True,\n",
        "      fill_mode='nearest')\n",
        "test_datagen = ImageDataGenerator(rescale=1./255,\n",
        "      rotation_range=40,\n",
        "      width_shift_range=0.2,\n",
        "      height_shift_range=0.2,\n",
        "      shear_range=0.2,\n",
        "      zoom_range=0.2,\n",
        "      horizontal_flip=True,\n",
        "      fill_mode='nearest')\n",
        "\n",
        "# Flow training images in batches of 20 using train_datagen generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,  # This is the source directory for training images\n",
        "        target_size=(224, 224),  # All images will be resized to 150x150\n",
        "        batch_size=1,\n",
        "        # Since we use binary_crossentropy loss, we need binary labels\n",
        "        class_mode='categorical')\n",
        "\n",
        "# Flow validation images in batches of 20 using test_datagen generator\n",
        "validation_generator = test_datagen.flow_from_directory(\n",
        "        validation_dir,\n",
        "        target_size=(224, 224),\n",
        "        batch_size=1,\n",
        "        class_mode='categorical')\n",
        "\n",
        "history = model.fit(\n",
        "      train_generator,\n",
        "      steps_per_epoch=75,  # 2000 images = batch_size * steps\n",
        "      epochs=100,\n",
        "      validation_data=validation_generator,\n",
        "      validation_steps=40,  # 1000 images = batch_size * steps\n",
        "      verbose=2)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 75 images belonging to 4 classes.\n",
            "Found 40 images belonging to 4 classes.\n",
            "Epoch 1/36\n",
            "75/75 - 16s - loss: 2.3987 - accuracy: 0.4533 - val_loss: 1.7600 - val_accuracy: 0.3500\n",
            "Epoch 2/36\n",
            "75/75 - 15s - loss: 0.8007 - accuracy: 0.6000 - val_loss: 1.6174 - val_accuracy: 0.4500\n",
            "Epoch 3/36\n",
            "75/75 - 15s - loss: 0.5876 - accuracy: 0.6933 - val_loss: 1.3398 - val_accuracy: 0.6000\n",
            "Epoch 4/36\n",
            "75/75 - 15s - loss: 0.7025 - accuracy: 0.7200 - val_loss: 0.8816 - val_accuracy: 0.7250\n",
            "Epoch 5/36\n",
            "75/75 - 15s - loss: 0.5988 - accuracy: 0.7733 - val_loss: 0.9722 - val_accuracy: 0.7000\n",
            "Epoch 6/36\n",
            "75/75 - 15s - loss: 0.4889 - accuracy: 0.8533 - val_loss: 0.6906 - val_accuracy: 0.6500\n",
            "Epoch 7/36\n",
            "75/75 - 15s - loss: 0.4628 - accuracy: 0.8267 - val_loss: 1.1069 - val_accuracy: 0.6500\n",
            "Epoch 8/36\n",
            "75/75 - 15s - loss: 0.3663 - accuracy: 0.8667 - val_loss: 0.9044 - val_accuracy: 0.7250\n",
            "Epoch 9/36\n",
            "75/75 - 15s - loss: 0.2408 - accuracy: 0.8933 - val_loss: 1.3011 - val_accuracy: 0.7000\n",
            "Epoch 10/36\n",
            "75/75 - 15s - loss: 0.1592 - accuracy: 0.9467 - val_loss: 1.1160 - val_accuracy: 0.7250\n",
            "Epoch 11/36\n",
            "75/75 - 15s - loss: 0.1880 - accuracy: 0.9600 - val_loss: 0.9725 - val_accuracy: 0.8000\n",
            "Epoch 12/36\n",
            "75/75 - 15s - loss: 0.1437 - accuracy: 0.9467 - val_loss: 1.0616 - val_accuracy: 0.7750\n",
            "Epoch 13/36\n",
            "75/75 - 15s - loss: 0.4935 - accuracy: 0.8933 - val_loss: 1.0869 - val_accuracy: 0.7500\n",
            "Epoch 14/36\n",
            "75/75 - 15s - loss: 0.5412 - accuracy: 0.8133 - val_loss: 1.3905 - val_accuracy: 0.6750\n",
            "Epoch 15/36\n",
            "75/75 - 15s - loss: 0.2834 - accuracy: 0.9067 - val_loss: 0.5649 - val_accuracy: 0.7500\n",
            "Epoch 16/36\n",
            "75/75 - 15s - loss: 0.1776 - accuracy: 0.9467 - val_loss: 1.4053 - val_accuracy: 0.7750\n",
            "Epoch 17/36\n",
            "75/75 - 15s - loss: 0.2898 - accuracy: 0.9200 - val_loss: 1.1078 - val_accuracy: 0.7750\n",
            "Epoch 18/36\n",
            "75/75 - 15s - loss: 0.1375 - accuracy: 0.9467 - val_loss: 2.1962 - val_accuracy: 0.6500\n",
            "Epoch 19/36\n",
            "75/75 - 15s - loss: 0.1264 - accuracy: 0.9333 - val_loss: 0.7457 - val_accuracy: 0.8000\n",
            "Epoch 20/36\n",
            "75/75 - 15s - loss: 0.2564 - accuracy: 0.9067 - val_loss: 1.6158 - val_accuracy: 0.7000\n",
            "Epoch 21/36\n",
            "75/75 - 15s - loss: 0.1686 - accuracy: 0.9600 - val_loss: 0.9064 - val_accuracy: 0.8750\n",
            "Epoch 22/36\n",
            "75/75 - 15s - loss: 0.2198 - accuracy: 0.9600 - val_loss: 1.2464 - val_accuracy: 0.7000\n",
            "Epoch 23/36\n",
            "75/75 - 15s - loss: 0.1326 - accuracy: 0.9467 - val_loss: 1.2466 - val_accuracy: 0.7250\n",
            "Epoch 24/36\n",
            "75/75 - 15s - loss: 0.0819 - accuracy: 0.9733 - val_loss: 1.5062 - val_accuracy: 0.6750\n",
            "Epoch 25/36\n",
            "75/75 - 15s - loss: 0.0737 - accuracy: 0.9733 - val_loss: 1.5702 - val_accuracy: 0.7250\n",
            "Epoch 26/36\n",
            "75/75 - 15s - loss: 0.0453 - accuracy: 0.9867 - val_loss: 1.6163 - val_accuracy: 0.7000\n",
            "Epoch 27/36\n",
            "75/75 - 15s - loss: 0.0715 - accuracy: 0.9867 - val_loss: 2.2223 - val_accuracy: 0.6000\n",
            "Epoch 28/36\n",
            "75/75 - 15s - loss: 0.0586 - accuracy: 0.9867 - val_loss: 2.0500 - val_accuracy: 0.7500\n",
            "Epoch 29/36\n",
            "75/75 - 15s - loss: 0.0380 - accuracy: 0.9867 - val_loss: 1.5978 - val_accuracy: 0.7000\n",
            "Epoch 30/36\n",
            "75/75 - 15s - loss: 0.0110 - accuracy: 1.0000 - val_loss: 1.5953 - val_accuracy: 0.7500\n",
            "Epoch 31/36\n",
            "75/75 - 15s - loss: 0.0992 - accuracy: 0.9733 - val_loss: 1.7022 - val_accuracy: 0.7250\n",
            "Epoch 32/36\n",
            "75/75 - 15s - loss: 0.0837 - accuracy: 0.9733 - val_loss: 1.2110 - val_accuracy: 0.7750\n",
            "Epoch 33/36\n",
            "75/75 - 15s - loss: 0.1872 - accuracy: 0.9733 - val_loss: 1.3904 - val_accuracy: 0.7750\n",
            "Epoch 34/36\n",
            "75/75 - 15s - loss: 0.0528 - accuracy: 0.9733 - val_loss: 1.5343 - val_accuracy: 0.7000\n",
            "Epoch 35/36\n",
            "75/75 - 15s - loss: 0.0055 - accuracy: 1.0000 - val_loss: 1.8126 - val_accuracy: 0.7250\n",
            "Epoch 36/36\n",
            "75/75 - 15s - loss: 0.0114 - accuracy: 1.0000 - val_loss: 1.3338 - val_accuracy: 0.6750\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tx1L7fxxWA_G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03a66a95-7fea-4d66-9696-2b7fc530d027"
      },
      "source": [
        "for image_batch, label_batch in train_generator:\n",
        "  break\n",
        "image_batch.shape, label_batch.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1, 224, 224, 3), (1, 4))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrFFcwUb3iK9"
      },
      "source": [
        "Save the labels in a file which will be downloaded later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QFZIhWs4dsq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5b09cd9-da7f-4559-dcca-be1c50a8a65d"
      },
      "source": [
        "print (train_generator.class_indices)\n",
        "\n",
        "labels = '\\n'.join(sorted(train_generator.class_indices.keys()))\n",
        "\n",
        "with open('labels.txt', 'w') as f:\n",
        "  f.write(labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'Akhil': 0, 'Deepesh': 1, 'Harsh': 2, 'Mohit': 3}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duxD_UDSOmng",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0c3b096-0a7d-4f7f-a1fd-e4f243ab7732"
      },
      "source": [
        "!cat labels.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Akhil\n",
            "Deepesh\n",
            "Harsh\n",
            "Mohit"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlx56nQtfe8Y"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8ARiyMFsgbH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd1a8405-0462-4202-bb58-ba68cb620d8b"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "mobilenetv2_1.00_224 (Functi (None, 7, 7, 1280)        2257984   \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 5, 5, 32)          368672    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 5, 5, 32)          0         \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_1 ( (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 4)                 132       \n",
            "=================================================================\n",
            "Total params: 2,626,788\n",
            "Trainable params: 368,804\n",
            "Non-trainable params: 2,257,984\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "krvBumovycVA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "774d3716-d5e5-48cd-cd0b-3866a6f11a21"
      },
      "source": [
        "print('Number of trainable variables = {}'.format(len(model.trainable_variables)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of trainable variables = 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRDabW_u1wnv"
      },
      "source": [
        "## Convert to TFLite"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNvMl6CM6lG4"
      },
      "source": [
        "Saved the model using `tf.saved_model.save` and then convert the saved model to a tf lite compatible format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LZiKVInWNGy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8292142-8077-4405-d376-c1ce692ab6c4"
      },
      "source": [
        "saved_model_dir = 'save/fine_tuning'\n",
        "tf.saved_model.save(model, saved_model_dir)\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "with open('model.tflite', 'wb') as f:\n",
        "  f.write(tflite_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: save/fine_tuning/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: save/fine_tuning/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GE4w-9S410Dk"
      },
      "source": [
        "Download the converted model and labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x47uW_lI1DoV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "4dae9a16-2acb-472e-cf52-a1921c2696eb"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('model.tflite')\n",
        "files.download('labels.txt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_48a6bf17-1827-4c57-90f1-6c4a85556397\", \"model.tflite\", 10378616)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_5cde366d-843b-436d-9e97-34ff014b4195\", \"labels.txt\", 25)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYtXacgcyQWM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}